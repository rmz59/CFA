\documentclass{article}
\usepackage[top=1.2in, bottom=1in, left=1in, right = 1in]{geometry}
%\usepackage[font=footnotesize,width = 0.8\textwidth]{caption}
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{indentfirst}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage[inline]{enumitem}   % This allows me to enumerate things inline.
\usepackage{hyperref}

\hypersetup{
    pdftitle={CFA II Notes},
    pdfauthor={Runmin Zhang},
    %pdfsubject={Your subject here},
    %pdfkeywords={keyword1, keyword2},
    bookmarksnumbered=true,     
    bookmarksopen=true,         
    bookmarksopenlevel=1,       
    colorlinks=true,            
    pdfstartview=Fit,           
    pdfpagemode=UseOutlines,    % this is the option you were lookin for
    pdfpagelayout=TwoPageRight
}


\setlength\parindent{0pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{CFA II Notes}
\fancyhead[R]{Runmin Zhang}
\fancyfoot[R]{\thepage}
\begin{document}
\tableofcontents

\section{Reading 9: Correlation and Regressions}
\subsection{Sample covar and sample correlation coefficients}
Sample covariance: $cov_{x,y}=\sum_i \frac{(X_i-\bar X)(Y_i-\bar Y)}{n-1}$
\\Sample correlation coeff: 
$r_{x,y}=\frac{cov_{x,y}}{s_x s_y}$, where $s_x$ is the sample dev of X.
\subsection{Limtations to correlations analysis}
Outliers: The results will be affected by extreme data points.(outliers)\\
Spurious correlation: There might be some non-zero corrlation coeff, but 
acutally they have no correlation at all.\\
Nonlinear relationships: Correlation only describe the linear relastions.
\subsection{Hypothesis: determine if the population 
correlation coefficient is zero}
Two-tailed hypothesis test:
$$
H_0: \rho=0, H_a: \rho \neq 0
$$
Assume that the two populations are {\bf normally} distrubited, then we 
can use t-test:
$$
t=\frac{r\sqrt{n-2}}{1-r^2}
$$:
Reject $H_0$ if  $t>+t_{critical}$ or $t<-t_{critical}$. Here,$r$ is the
sample correlation. Remember, you need to check t-table to find the t-value.
\subsection{Determine dependent/indepedent variables in a linear regression}
{\bf Simple linear regression}: Explain the variation in a dependent variable 
in terms of the variabltion in a single indepedent variable.
{\bf Independent variables} are called explanatory variable, the exogenous 
variable, or the predicting variable. 
{\bf Dependent variable} is also called the explained variable, the endogenous 
variable, or the predicted variable.
\subsection{Assumptions in linear regression and interpret regression coeff.}
\begin{enumerate}
    \item Assumptions of linear regression:
        \begin{enumerate}
            \item Linear relationship must exist.
            \item The indepedent variable is uncorrelated with residuals.
            \item Expected Residual term is value. $E(\epsilon)=0$
            \item variance of the residual term is const. $E(\epsilon_i^2)=
                \sigma_\epsilon^2$
            \item The residual term is independently distributed. 
                $E(\epsilon_i\epsilon_j)=1$    
            \item The residual term is normally distributed.
        \end{enumerate}
    \item Simple Linear Regression Model
        \begin{enumerate}
            \item Model: $Y_i=b_0+b_1X_i+\epsilon_i$, where $i=1...n$, and $Y_i$ is 
     the actual observed data.
            \item The fitted line, the line of best fit
                : $\hat{Y}=\hat{b_0}+\hat{b1}X_i$. Where $\hat{b_0}$
                is the estimated parameter of the model.
            \item How to choose the best fitted line? {\bf Sum of squared errors}
                 is minimum.
                 $$
                    \hat{b_1} = \frac{cov_{x,y}}{sigma_x^2}
                 $$
                 where $X$ is the indepdent variable. $\hat{b_1}$ is 
                 "regression coeffcient".
                 $$
                    \hat{b_0} = \bar Y - \hat{b_1}\bar X
                 $$
                 where $\bar X, \bar Y$ are the mean.
        \end{enumerate}
    \item Interpreting a regression coefficient: Similar to basic ideas of 
        "slope". Keep in mind: any conclusion regarding this parameter needs 
        the statistical significance of the slope coefficient.
\end{enumerate}
\subsection{Standard error of estimate, the coeff. of determination and a 
confidence interval for a regression coefficient.}
\begin{enumerate}
    \item Standard error of estimate (SEE): Standard deviation between $Y_{estimate}$
        and $Y_{actual}$. - Smaller: better
    \item Coefficient of Determination ($R^2$)
        The percentage of the total variance in the dependent variable that is 
        predictable from the indepedent variable. - One indepdent variable: $R^2=r^2$,
        where $r^2$ is the square of correlation coefficient.
    \item Regression Coefficient confidence interval
        \begin{enumerate}
            \item Hypothesis: $H_0: b_1=0 \Leftrightarrow H_a: b_1\neq 0$ 
            \item Confidence interval:
                $\hat{b_1}-(t_c s_{\hat{b_1}})<b_1<\hat{b_1}+(t_c s_{\hat{b_1}})$ 
            $s_{\hat{b}_1}$ is the standard error of the regression coeffi.  
        \end{enumerate}
\end{enumerate}
\subsection{Hypothesis: Determine if $\hat{b}_1=b_1$}
\begin{enumerate}
    \item t-test statistic: $t_{b_1}=\frac{\hat{b}_1 - b_1}{s_{\hat{b}_1}}$
    \item Reject: if $t>+t_{critical}$ or $t<-t_{critical}$
\end{enumerate}
\subsection{Calculate the predicted value for the depedent variable}
If an estimated regression model is known, $\hat{Y}=\hat{b}_0+\hat{b}_1 X_p$
\subsection{Calculate and interpret a confidence interval for the predicted 
value of the depedent variable}
\begin{enumerate}
    \item Eq: $\hat{Y}\pm(t_c s_f)$, where $s_f$ is the {\color{red}std error of the forecast.}
    \item $s_f^2=SEE^2 \left[1+\frac{1}{n}+\frac{(X-\bar X)^2}{(n-1)s_x^2} \right]$
        \begin{enumerate}
            \item $SEE^2=$ variance of the residuals
            \item $s_x^2=$ variance of the indepdent variable
            \item $X =$ value of the independent variable where the forecast was
                made.
        \end{enumerate}
\end{enumerate}
\subsection{ANOVA in regression. Interpret results, and calculate F-statistic}
\begin{enumerate}
    \item Analysis of variance (ANOVA) is used to analyze the total variability 
        of the depedent variable. 
        \begin{enumerate}
            \item Total sum of squares(SST): $SST=\sum_{i=1}^n(Y_i-\bar Y)^2$
                \\SST is the total variation in the depedent variable.
                $Variance=SST/(n-1)$
            \item Regression sum of squares(RSS): $RSS=\sum_{i=1}^n(\hat{Y}_i-\bar Y)^2$
                \\RSS is the explained variation.
            \item Sum of squared errors(SSE): $SSE=\sum_{i=1}^n(Y_i-\hat{Y}_i)^2$
                \\SSE is the unexplained variation.
            \item {\color{red}$SST = RSS+SSE$ I cannot get this equation yet}
                You need to know how to use these squares.
            \item Degree of freedom: i) Regression(Explained): $k=1$, since we only
                estimate one parameters.
                ii) Error(Unexplained) $df=n-k-1=n-2$
                iii) Total variation $df=n-1$
        \end{enumerate}
    \item Calculating $R^2$ and {\bf SEE}
        \begin{enumerate}
            \item $R^2=explained variation/total varn=RSS/SST$
            \item $\bf{SEE}=\sqrt\frac{SSE}{n-2}$ {\bf SEE} is the std deviation of the
                regression error terms.
        \end{enumerate}
    \item The F-Statistic: used to explain whether {\it at least one} indepdent parameter
        can significanly explain the dependent parameter.  
        \begin{enumerate}
            \item F-statistic eq: $F=\frac{MSR}{MSE}=\frac{RSS/k}{SSE/n-k-1}$
                 where $MSR=$ mean regression sum of squares.
                       $MSE=$ mean squared errors. 
                       Note: \color{red}{One tailed test!}
        \end{enumerate}
    \item F-statistic with one independent variable.
        \begin{enumerate}
            \item Hypothesis: $H_0: b1=0 \Leftrightarrow H_a: b1\ne 0$
            \item degree of freedom: $df_{rss}=k=1,df_{sse}=n-k-1$
            \item Decision rule: reject $H_0$ if $F>F_c$
        \end{enumerate}
\end{enumerate}
\subsection{Limitations of regression analysis}
    \begin{enumerate}
        \item Parameter instability: the estimation eq may not be useful for other times.
        \item Limited usefulness: other participants may also use the same eq.
        \item Assumptions does not hold: i) Heteroskedastic, i.e., non-const
            variance of the error terms. ii) autocorrelation, i.e., error terms
            are not independent.
    \end{enumerate}
\end{document}
